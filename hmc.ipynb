{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import scipy.io, scipy.interpolate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* $\\theta$: the parameters we're trying to infer; in David's model, that's either the accumulation or velocity scaling factors\n",
    "* $\\phi$: the \"momentum\", you'll see what I mean in a bit\n",
    "* $P(\\theta)$: the probability density function of the parameters\n",
    "* $\\tau$: a fictitious, made-up time (ex: the lifespan of RBG, clearly she's immortal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review & motivation\n",
    "\n",
    "Say we want to sample from some probability density $P(\\theta)$.\n",
    "There are two parts to MCMC sampling:\n",
    "\n",
    "* The **transition probability** $Q(\\theta_{k + 1} | \\theta_k)$: given a state $\\theta_k$, the next candidate $\\theta_{k + 1}$ is generated by sampling from $Q(\\theta | \\theta_k)$.\n",
    "* The **accept/reject** criterion: accept $\\theta_{k + 1}$ with probability $\\min(1, P(\\theta_{k + 1})/P(\\theta_k))$.\n",
    "\n",
    "Usually, $Q(\\theta_{k + 1} | \\theta_k)$ = normal distribution with mean $\\theta_k$.\n",
    "But this choice is arbitrary and we could use anything, so long as the transitions are reversible: $Q(\\theta' | \\theta) = Q(\\theta | \\theta')$.\n",
    "\n",
    "**Hamiltonian Monte Carlo is a clever choice of transition probability.**\n",
    "Rather than try to guess randomly where to go from the current parameters, HMC uses information from the contours/gradient of the density $P$ (really $-\\log P$) to guide the search.\n",
    "\n",
    "Lots of algorithms (stochastic gradient descent, simulated annealing, etc.) go down-gradient to find modes of $P$.\n",
    "This is essentially like solving the ODE\n",
    "\n",
    "$$\\frac{d}{d\\tau}\\theta - \\nabla\\log P = 0.$$\n",
    "\n",
    "The solutions of this ODE as the fictitious time $\\tau$ goes to $\\infty$ go to extrema.\n",
    "As we've seen this tends to get stuck in local extrema and you need tricks to make it explore everywhere.\n",
    "HMC instead uses a second-order ODE\n",
    "\n",
    "$$\\frac{d^2}{d\\tau^2}\\theta - \\nabla\\log P = 0$$\n",
    "\n",
    "to make the guesses oscillate around the modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian mechanics\n",
    "\n",
    "**Hamiltonian mechanics** is a particular way of describing classical physical systems.\n",
    "\n",
    "* **The players**: position $\\theta$, momentum $\\phi$, and the total energy $H(\\theta, \\phi)$ of the system\n",
    "* **The rules**: Hamilton's equations of motion,\n",
    "$$\\begin{align}\n",
    "\\dot \\theta & = +\\frac{\\partial H}{\\partial \\phi} \\\\\n",
    "\\dot \\phi & = -\\frac{\\partial H}{\\partial \\theta}\n",
    "\\end{align}$$\n",
    "* When $\\phi = m\\dot \\theta$, and $H = $ kinetic energy + potential energy, Hamilton's equations of motion are equivalent to Newton's.\n",
    "\n",
    "Some very important things:\n",
    "* The energy $H$ is conserved along trajectories of the ODE.\n",
    "* The volume in phase space is conserved.\n",
    "Take a \"blob\" $D$ of position/momentum pairs, now evolve them all for a time $\\tau$ using Hamilton's equations; this gives a morphed blob, $D_\\tau$.\n",
    "Then $\\mathrm{vol}(D) = \\mathrm{vol}(D_\\tau)$.\n",
    "* Hamiltonian dynamics is *reversible* -- flipping all the momenta $\\Leftrightarrow$ going backwards in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo\n",
    "\n",
    "MCMC simulation works with any reversible transition kernel.\n",
    "The idea of HMC is to augment the state $\\theta$ with a *pseudo-momentum* variable $\\phi$ and use Hamiltonian dynamics to update both $\\theta$ and $\\phi$.\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "0. Pick a starting parameter $\\theta_0$ however you like.\n",
    "1. Pick a starting momentum $\\phi_0$ sampled from a normal distribution.\n",
    "2. Solve Hamilton's equation's of motion up to some time $\\tau$.\n",
    "3. Accept/reject the new state $\\theta_\\tau, \\phi_\\tau$ according to the usual MCMC criterion:\n",
    "$$\\min\\{1, \\exp\\left[H(\\theta_0, \\phi_0) - H(\\theta_\\tau, \\phi_\\tau)\\right]\\}$$\n",
    "4. Throw out the momentum and go back to (1).\n",
    "\n",
    "If we were able to solve Hamilton's equations exactly, the energy would be conserved and we would always accept the new state!\n",
    "\n",
    "You can think of HMC as a way to sample from the PDF at two distinct levels at each step.\n",
    "* Random magnitude of $\\phi$ $\\Rightarrow$ sampling *across* energy contours.\n",
    "* Random direction of $\\phi$ + solving Hamilton's equations $\\Rightarrow$ sampling *along* energy contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some real data\n",
    "\n",
    "Let's try and apply this to David's data from last week.\n",
    "We're going to do a little more cleaning up to remove duplicate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = scipy.io.loadmat('layers.mat')\n",
    "sx, sy = layers['psx_layers'][0], layers['psy_layers'][0]\n",
    "dx = np.sqrt(np.diff(sx)**2 + np.diff(sy)**2)\n",
    "x = np.hstack(([0], np.cumsum(dx)))[:5000]\n",
    "target_layer = layers['layer_14'][0][:5000]\n",
    "\n",
    "# Find any points that are duplicated or where there's data missing and remove them\n",
    "repeat_point_indices = np.where(dx < 1.0)[0] + 1\n",
    "no_data_indices = np.where(np.isnan(target_layer))[0]\n",
    "good_indices = set(range(5000)) - set(repeat_point_indices) - set(no_data_indices)\n",
    "indices = np.array(list(good_indices))\n",
    "indices.sort()\n",
    "\n",
    "x = x[indices]\n",
    "target_layer = target_layer[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = scipy.io.loadmat('vels.mat')\n",
    "vel_interpolater = scipy.interpolate.interp1d(more_data['dists'][0], more_data['vels'][0])\n",
    "u = vel_interpolater(x)\n",
    "acc_interpolater = scipy.interpolate.interp1d(more_data['acc_dists'][0], more_data['acc'][0])\n",
    "a = acc_interpolater(x) * 1.4  # need a little cheating to help us along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the present-day accumulation rate $a(x)$, ice velocity $u(x)$, and the scaling factors $s_a(t)$, $s_u(t)$, David's model for the layer depth $z$ is the solution of the advection equation\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial t} - s_u(t)u(x)\\frac{\\partial z}{\\partial x} = s_a(t) \\cdot DJ(a, z),$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x}\\Big|_{x = L} = 0.$$\n",
    "\n",
    "up to the final time $t_f$, where $DJ$ is the Dansgaard-Johnsen model for the vertical velocity.\n",
    "David used an arbitrary Lagrangian-Eulerian scheme to solve these equations.\n",
    "\n",
    "**One of the hard parts of using HMC is calculating derivatives of the objective w.r.t. the parameters.**\n",
    "To be able to calculate derivatives, I had to use a more old-fashioned backward Euler integration scheme to solve the model physics.\n",
    "I calculated the derivatives using the adjoint method.\n",
    "The derivatives were very inaccurate without a small timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "total_time = 725.0\n",
    "num_steps = 40\n",
    "\n",
    "accumulation_scale = 1.08 * np.ones(num_steps)\n",
    "velocity_scale = 1.05 * np.ones(num_steps)\n",
    "\n",
    "z = model.layer_depth(x, accumulation_scale, velocity_scale,\n",
    "                      a, u, total_time, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x/1000, target_layer, color='k', label='Data')\n",
    "ax.plot(x/1000, z[num_steps - 1, :], color='b', label='Model')\n",
    "ax.set_xlabel(\"x (km)\")\n",
    "fig.legend()\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative of the model-data misfit with respect to the input parameters, I used the adjoint method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJ_da = model.derivative_ascale(x, accumulation_scale, velocity_scale,\n",
    "                                a, u, z, target_layer,\n",
    "                                total_time, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the sensitivity of the mean-square error to changes in the accumulation scale over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = total_time / num_steps\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(range(num_steps)) * dt, dJ_da, color='k')\n",
    "ax.set_xlabel('t (years)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the expected decrease in the mean-square error, assuming that we perturbed the accumulation scale by going down-gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = -dJ_da\n",
    "dJ = np.dot(dJ_da, da) * dt\n",
    "print(\"{:e}\".format(dJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure I implemented everything right, we can calculate the actual decrease in the error and see how well it agrees with the value `dJ` computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_0 = model.mean_square_misfit(x, z[-1, :], target_layer)\n",
    "print(\"{}\".format(error_0))\n",
    "\n",
    "for k in range(20):\n",
    "    δ = 1.0/2**k\n",
    "    zk = model.layer_depth(x, accumulation_scale + δ * da, velocity_scale,\n",
    "                           a, u, total_time, num_steps)\n",
    "    error = model.mean_square_misfit(x, zk[-1, :], target_layer)\n",
    "    \n",
    "    print(\"{}, {}\".format((error - error_0)/δ, dJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't too bad given the discretization.\n",
    "I might still have made an error somewhere -- the velocity derivatives, while in the right direction, are usually off by a factor of 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton of HMC\n",
    "\n",
    "We now have everything in place to start sampling from the PDF for David's data and model!\n",
    "\n",
    "In order to make the runs reproducible, we'll use a `RandomState` object from numpy for all our random number generation.\n",
    "If some run is particularly interesting, we can serialize the random state object, save it to disk, and deserialize it later when we want to reproduce the run identically.\n",
    "This involves some extra book-keeping but you should always do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = random.RandomState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we discussed using regularization in order to select for smooth values of the parameters; this is equivalent to a certain choice of prior in Bayesian inference.\n",
    "We'll do the same thing this time.\n",
    "**How you regularize the problem determines the choice of mass matrix**, i.e. how the different components of the momentum vector are coupled together.\n",
    "\n",
    "Note how we divide by the square root of the number of time steps to get the right total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 20\n",
    "import scipy.sparse, scipy.sparse.linalg, numpy.linalg\n",
    "Id = scipy.sparse.diags([np.ones(num_steps)], [0])\n",
    "diag = np.ones(num_steps)\n",
    "D = scipy.sparse.diags([diag, -diag], [0, 1], shape=(39, 40)) / dt\n",
    "L = r**2 * D.T * D * dt\n",
    "M = Id + L\n",
    "\n",
    "λs, vs = numpy.linalg.eigh(M.toarray())\n",
    "\n",
    "def generate_momentum(random_state, σ):\n",
    "    ζ = random_state.normal(size=num_steps)\n",
    "    return sum(((ζ[k] * np.sqrt(λs[k] / num_steps) * σ) * vs[:, k] for k in range(num_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions calculate each part of the Hamiltonian and its derivatives.\n",
    "The final term in the potential -- `np.dot(θ, L * θ)` -- is what adds the penalty for oscillatory solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_energy(θ):\n",
    "    z_θ = model.layer_depth(x, θ, velocity_scale,\n",
    "                            a, u, total_time, num_steps)\n",
    "    return model.mean_square_misfit(x, z_θ[-1,:], target_layer) + \\\n",
    "           0.5 * np.dot(θ, L * θ)\n",
    "\n",
    "def kinetic_energy(ϕ):\n",
    "    return 0.5 * np.dot(ϕ, scipy.sparse.linalg.spsolve(M, ϕ))\n",
    "\n",
    "def force(θ):\n",
    "    z_θ = model.layer_depth(x, θ, velocity_scale,\n",
    "                            a, u, total_time, num_steps)\n",
    "    return -model.derivative_ascale(x, θ, velocity_scale,\n",
    "                                    a, u, z_θ, target_layer,\n",
    "                                    total_time, num_steps) - L * θ\n",
    "\n",
    "def velocity(ϕ):\n",
    "    return scipy.sparse.linalg.spsolve(M, ϕ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements the Verlet method for numerically integrating a Hamiltonian system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamiltonian_update(δτ, θ, ϕ):\n",
    "    θ_τ = θ + 0.5 * δτ * velocity(ϕ)\n",
    "    ϕ_τ = ϕ + δτ * force(θ)\n",
    "    θ_τ += 0.5 * δτ * velocity(ϕ_τ)\n",
    "    return θ_τ, ϕ_τ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the initial parameter and momentum and plot the velocity.\n",
    "The initial kinetic energy should be around 0.5 (it's a $\\chi^2$ random variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = accumulation_scale.copy()\n",
    "ϕ = generate_momentum(random_state, 1)\n",
    "\n",
    "print(kinetic_energy(ϕ))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(range(num_steps)) * dt, velocity(ϕ), color='k')\n",
    "ax.set_xlabel('t (years)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the system using a fictitious timestep of 1/32 for 2/3 of a fictitious time unit.\n",
    "I had to hand-tune both of these parameters to get something sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δτ = 1.0/32\n",
    "num_hamiltonian_steps = int(2.0 / 3 / δτ)\n",
    "\n",
    "for k in range(num_hamiltonian_steps):\n",
    "    θ, ϕ = hamiltonian_update(δτ, θ, ϕ)\n",
    "    print(kinetic_energy(ϕ), potential_energy(θ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a few statistics of the resulting random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(θ), np.max(θ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kinetic_energy(ϕ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(range(num_steps)) * dt, θ, color='k')\n",
    "ax.set_xlabel('t (years)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(potential_energy(θ), potential_energy(accumulation_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting layer depth undershoots pretty dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_θ = model.layer_depth(x, θ, velocity_scale,\n",
    "                        a, u, total_time, num_steps)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x/1000, target_layer, color='k', label='Data')\n",
    "ax.plot(x/1000, z_θ[num_steps - 1, :], color='b', label='Model')\n",
    "ax.set_xlabel(\"x (km)\")\n",
    "fig.legend()\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMC sampling\n",
    "\n",
    "Now let's generate a bunch of samples and see what the resulting ensemble looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "θs = np.zeros((num_samples, num_steps))\n",
    "θs[0, :] = accumulation_scale.copy()\n",
    "for sample in range(1, num_samples):\n",
    "    ϕ = generate_momentum(random_state, 1)\n",
    "    θ = θs[sample - 1, :].copy()\n",
    "\n",
    "    for k in range(num_hamiltonian_steps):\n",
    "        θ, ϕ = hamiltonian_update(δτ, θ, ϕ)\n",
    "        \n",
    "    print(kinetic_energy(ϕ), potential_energy(θ))\n",
    "    θs[sample, :] = θ.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lighter colors are earlier runs, darker colors are later ones.\n",
    "We can clearly see the guesses migrating through parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.get_cmap('binary')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for sample in range(num_samples):\n",
    "    ax.plot(np.array(range(num_steps)) * dt, θs[sample, :],\n",
    "            color=colormap((sample + 1) / num_samples))\n",
    "ax.set_xlabel('t (years)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the differences in the parameter values, the resulting ice layers are very clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for sample in range(num_samples):\n",
    "    θ = θs[sample, :]\n",
    "    z_θ = model.layer_depth(x, θ, velocity_scale,\n",
    "                            a, u, total_time, num_steps)\n",
    "    ax.plot(x/1000, z_θ[-1, :], color=colormap((sample + 1) / num_samples))\n",
    "    ax.plot()\n",
    "    \n",
    "ax.plot(x/1000, target_layer, color='r')\n",
    "plt.show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
