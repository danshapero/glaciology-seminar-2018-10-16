{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import scipy.io, scipy.interpolate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* $\\theta$: the parameters we're trying to infer; in David's model, that's either the accumulation or velocity scaling factors\n",
    "* $\\phi$: the \"momentum\", you'll see what I mean in a bit\n",
    "* $P(\\theta)$: the probability density function of the parameters\n",
    "* $\\tau$: a fictitious, made-up time (ex: the lifespan of RBG, clearly she's immortal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review & motivation\n",
    "\n",
    "Say we want to sample from some probability density $P(\\theta)$.\n",
    "There are two parts to MCMC sampling:\n",
    "\n",
    "* The **transition probability** $Q(\\theta_{k + 1} | \\theta_k)$: given a state $\\theta_k$, the next candidate $\\theta_{k + 1}$ is generated by sampling from $Q(\\theta | \\theta_k)$.\n",
    "* The **accept/reject** criterion: accept $\\theta_{k + 1}$ with probability $\\min(1, P(\\theta_{k + 1})/P(\\theta_k))$.\n",
    "\n",
    "Usually, $Q(\\theta_{k + 1} | \\theta_k)$ = normal distribution with mean $\\theta_k$.\n",
    "But this choice is arbitrary and we could use anything, so long as the transitions are reversible: $Q(\\theta' | \\theta) = Q(\\theta | \\theta')$.\n",
    "\n",
    "**Hamiltonian Monte Carlo is a clever choice of transition probability.**\n",
    "Rather than try to guess randomly where to go from the current parameters, HMC uses information from the contours/gradient of the density $P$ (really $-\\log P$) to guide the search.\n",
    "\n",
    "Lots of algorithms (stochastic gradient descent, simulated annealing, etc.) go down-gradient to find modes of $P$.\n",
    "This is essentially like solving the ODE\n",
    "\n",
    "$$\\frac{d}{d\\tau}\\theta - \\nabla\\log P = 0.$$\n",
    "\n",
    "The solutions of this ODE as the fictitious time $\\tau$ goes to $\\infty$ go to extrema.\n",
    "As we've seen this tends to get stuck in local extrema and you need tricks to make it explore everywhere.\n",
    "HMC instead uses a second-order ODE\n",
    "\n",
    "$$\\frac{d^2}{d\\tau^2}\\theta - \\nabla\\log P = 0$$\n",
    "\n",
    "to make the guesses oscillate around the modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian mechanics\n",
    "\n",
    "**Hamiltonian mechanics** is a particular way of describing classical physical systems.\n",
    "\n",
    "* **The players**: position $\\theta$, momentum $\\phi$, and the total energy $H(\\theta, \\phi)$ of the system\n",
    "* **The rules**: Hamilton's equations of motion,\n",
    "$$\\begin{align}\n",
    "\\dot \\theta & = +\\frac{\\partial H}{\\partial \\phi} \\\\\n",
    "\\dot \\phi & = -\\frac{\\partial H}{\\partial \\theta}\n",
    "\\end{align}$$\n",
    "* When $\\phi = m\\dot \\theta$, and $H = $ kinetic energy + potential energy, Hamilton's equations of motion are equivalent to Newton's.\n",
    "\n",
    "Some very important things:\n",
    "* The energy $H$ is conserved along trajectories of the ODE.\n",
    "* The volume in phase space is conserved.\n",
    "Take a \"blob\" $D$ of position/momentum pairs, now evolve them all for a time $\\tau$ using Hamilton's equations; this gives a morphed blob, $D_\\tau$.\n",
    "Then $\\mathrm{vol}(D) = \\mathrm{vol}(D_\\tau)$.\n",
    "* Hamiltonian dynamics is *reversible* -- flipping all the momenta $\\Leftrightarrow$ going backwards in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo\n",
    "\n",
    "MCMC simulation works with any reversible transition kernel.\n",
    "The idea of HMC is to augment the state $\\theta$ with a *pseudo-momentum* variable $\\phi$ and use Hamiltonian dynamics to update both $\\theta$ and $\\phi$.\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "0. Pick a starting parameter $\\theta_0$ however you like.\n",
    "1. Pick a starting momentum $\\phi_0$ sampled from a normal distribution.\n",
    "2. Solve Hamilton's equation's of motion up to some time $\\tau$.\n",
    "3. Accept/reject the new state $\\theta_\\tau, \\phi_\\tau$ according to the usual MCMC criterion:\n",
    "$$\\min\\{1, \\exp\\left[H(\\theta_0, \\phi_0) - H(\\theta_\\tau, \\phi_\\tau)\\right]\\}$$\n",
    "4. Throw out the momentum and go back to (1).\n",
    "\n",
    "If we were able to solve Hamilton's equations exactly, the energy would be conserved and we would always accept the new state!\n",
    "\n",
    "You can think of HMC as a way to sample from the PDF at two distinct levels at each step.\n",
    "* Random magnitude of $\\phi$ $\\Rightarrow$ sampling *across* energy contours.\n",
    "* Random direction of $\\phi$ + solving Hamilton's equations $\\Rightarrow$ sampling *along* energy contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some real data\n",
    "\n",
    "Let's try and apply this to David's data from last week.\n",
    "We're going to do a little more cleaning up to remove duplicate points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = scipy.io.loadmat('layers.mat')\n",
    "sx, sy = layers['psx_layers'][0], layers['psy_layers'][0]\n",
    "dx = np.sqrt(np.diff(sx)**2 + np.diff(sy)**2)\n",
    "x = np.hstack(([0], np.cumsum(dx)))[:5000]\n",
    "target_layer = layers['layer_14'][0][:5000]\n",
    "\n",
    "# Find any points that are duplicated or where there's data missing and remove them\n",
    "repeat_point_indices = np.where(dx < 1.0)[0] + 1\n",
    "no_data_indices = np.where(np.isnan(target_layer))[0]\n",
    "good_indices = set(range(5000)) - set(repeat_point_indices) - set(no_data_indices)\n",
    "indices = np.array(list(good_indices))\n",
    "indices.sort()\n",
    "\n",
    "x = x[indices]\n",
    "target_layer = target_layer[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = scipy.io.loadmat('vels.mat')\n",
    "vel_interpolater = scipy.interpolate.interp1d(more_data['dists'][0], more_data['vels'][0])\n",
    "velocity = vel_interpolater(x)\n",
    "acc_interpolater = scipy.interpolate.interp1d(more_data['acc_dists'][0], more_data['acc'][0])\n",
    "accumulation = acc_interpolater(x) * 1.4  # need a little cheating to help us along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the present-day accumulation rate $a(x)$, ice velocity $u(x)$, and the scaling factors $s_a(t)$, $s_u(t)$, David's model for the layer depth $z$ is the solution of the advection equation\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial t} - s_u(t)u(x)\\frac{\\partial z}{\\partial x} = s_a(t) \\cdot DJ(a, z),$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x}\\Big|_{x = L} = 0.$$\n",
    "\n",
    "up to the final time $t_f$, where $DJ$ is the Dansgaard-Johnsen model for the vertical velocity.\n",
    "David used an arbitrary Lagrangian-Eulerian scheme to solve these equations.\n",
    "\n",
    "**One of the hard parts of using HMC is calculating derivatives of the objective w.r.t. the parameters.**\n",
    "To be able to calculate derivatives, I had to use a more old-fashioned backward Euler integration scheme to solve the model physics.\n",
    "I calculated the derivatives using the adjoint method.\n",
    "The derivatives were very inaccurate without a small timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "total_time = 725.0\n",
    "num_steps = 40\n",
    "\n",
    "accumulation_scale = 1.08 * np.ones(num_steps)\n",
    "velocity_scale = 1.05 * np.ones(num_steps)\n",
    "\n",
    "z = model.layer_depth(x, accumulation_scale, velocity_scale,\n",
    "                      accumulation, velocity, total_time, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x/1000, target_layer, color='k', label='Data')\n",
    "ax.plot(x/1000, z[num_steps - 1, :], color='b', label='Model')\n",
    "ax.set_xlabel(\"x (km)\")\n",
    "fig.legend()\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative of the model-data misfit with respect to the input parameters, I used the adjoint method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJ_da = model.derivative_ascale(x, accumulation_scale, velocity_scale,\n",
    "                                accumulation, velocity, z, target_layer,\n",
    "                                total_time, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the sensitivity of the mean-square error to changes in the accumulation scale over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = total_time / num_steps\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(range(num_steps)) * dt, dJ_da, color='k')\n",
    "ax.set_xlabel('t (years)')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the expected decrease in the mean-square error, assuming that we perturbed the accumulation scale by going down-gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = -dJ_da\n",
    "dJ = np.dot(dJ_da, da) * dt\n",
    "print(\"{:e}\".format(dJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure I implemented everything right, we can calculate the actual decrease in the error and see how well it agrees with the value `dJ` computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_0 = model.mean_square_misfit(x, z[-1, :], target_layer)\n",
    "print(\"{}\".format(error_0))\n",
    "\n",
    "for k in range(20):\n",
    "    δ = 1.0/2**k\n",
    "    zk = model.layer_depth(x, accumulation_scale + δ * da, velocity_scale,\n",
    "                           accumulation, velocity, total_time, num_steps)\n",
    "    error = model.mean_square_misfit(x, zk[-1, :], target_layer)\n",
    "    \n",
    "    print(\"{}, {}\".format((error - error_0)/δ, dJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't too bad given the discretization.\n",
    "I might still have made an error somewhere -- the velocity derivatives, while in the right direction, are usually off by a factor of 2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
